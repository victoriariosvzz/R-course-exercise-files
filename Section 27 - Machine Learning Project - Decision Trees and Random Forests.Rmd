---
title: "Section 27 - Machine Learning Project - Decision Trees and Random Forests"
author: "Victoria"
date: "13/1/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#**Tree Methods Project**
For this project we will be exploring the use of tree methods to classify schools as Private or Public based off their features.

Let's start by getting the data which is included in the ISLR library, the College data frame.
```{r}
library(ISLR)

head(College)

df <- College
colnames(df)
```

**EDA**
Let's explore the data!

Create a scatterplot of Grad.Rate versus Room.Board, colored by the Private column.
```{r}
library(ggplot2)

ggplot(df, aes(x = Room.Board, y = Grad.Rate)) + geom_point(aes(color = Private)) + theme_bw()
```

Create a histogram of full time undergrad students, color by Private.
```{r}
ggplot(df, aes(x = F.Undergrad)) + geom_histogram(aes(fill = Private), color = "black") + theme_bw()
```

Create a histogram of Grad.Rate colored by Private. You should see something odd here.
```{r}
ggplot(df, aes(x = Grad.Rate)) + geom_histogram(aes(fill = Private), color = "black") + theme_bw()
```
What college had a Graduation Rate of above 100% ?
```{r}
grad.above100 <- subset(df, Grad.Rate > 100)

print(grad.above100)
```


Change that college's grad rate to 100%
```{r}
df["Cazenovia College","Grad.Rate"] <- 100

df["Cazenovia College","Grad.Rate"]

```
**Train Test Split**
Split your data into training and testing sets 70/30. Use the caTools library to do this.

```{r}
library(caTools)

split <- sample.split(df, SplitRatio = 0.7)

train <- subset(df, split == TRUE)
test <- subset(df, split == FALSE)
```

**Decision Tree**
Use the rpart library to build a decision tree to predict whether or not a school is Private. Remember to only build your tree off the training data.
```{r}
library(rpart)

tree <- rpart(Private ~ ., method = "class", data = train)
```

Use predict() to predict the Private label on the test data.
```{r}
predicted.Private <- predict(tree, test)

```

Check the Head of the predicted values. You should notice that you actually have two columns with the probabilities.
```{r}
head(predicted.Private)
```

Turn these two columns into one column to match the original Yes/No Label for a Private column.
```{r}
predicted.Private <- as.data.frame(predicted.Private)

joiner <- function(x){
  if (x >= 0.5) {
    return("Yes")
  }else{
    return("No")
  }
}

predicted.Private$Private <- sapply(predicted.Private$Yes,joiner)

head(predicted.Private)
```

Now use table() to create a confusion matrix of your tree model.
```{r}
table(predicted.Private$Private, test$Private)
```
Use the rpart.plot library and the prp() function to plot out your tree model.
```{r}
library(rpart.plot)

prp(tree)
```
**Random Forest**
Now let's build out a random forest model!

Call the randomForest package library
```{r}
library(randomForest)
```

Now use randomForest() to build out a model to predict Private class. Add importance=TRUE as a parameter in the model. (Use help(randomForest) to find out what this does.
```{r}
rf.model <- randomForest(Private ~ ., data = train, importance = TRUE)

print(rf.model)
```

What was your model's confusion matrix on its own training set? Use model$confusion.
```{r}
rf.model$confusion
rf.model$importance
rf.model$predicted
```

**Predictions**
Now use your random forest model to predict on your test set!
```{r}
rf.preds <- predict(rf.model,test)

table(rf.preds, test$Private)

accuracy <- (53 + 187) / (53 + 9 + 11 + 187)
accuracy
```
It should have performed better than just a single tree, how much better depends on whether you are emasuring recall, precision, or accuracy as the most important measure of the model.
